{"cells": [{"cell_type": "markdown", "metadata": {"_cell_guid": "a550fbb8-895c-40ad-80e0-56b078e0c375", "_uuid": "156e57ac94d8b988126e48916329880a57a7d8e6"}, {"cell_type": "code", "metadata": {"_cell_guid": "4818579e-6989-413f-bb7b-4166e48cbe06", "collapsed": true, "_uuid": "a693f2c9e3b55e90eb78d92a55d3f8003ab48767"}, "outputs": [], "execution_count": null, "source": ["import tensorflow as tf\n", "import numpy as np"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "80dee786-7ab2-4ebd-ba50-5b130c672bec", "_uuid": "03158698f7e665e29ea145f6f4705ba2ba5441be"}, "source": ["# Functions"]}, {"cell_type": "code", "metadata": {"_cell_guid": "4083d6b9-1598-4dc6-9a92-6bb09443963a", "collapsed": true, "_uuid": "4523e52a45fea5881354e0ba63abc21b5a86906b"}, "outputs": [], "execution_count": null, "source": ["def get_s(b_prior, u_hat):\n", "    c = tf.nn.softmax(b_prior, axis=2)\n", "    s = tf.reduce_sum(tf.multiply(c, u_hat), axis=1)\n", "    return(s, c)\n", "\n", "#Squash\n", "def get_v(s):\n", "    norm_s = tf.norm(s, axis=2, keepdims=True)\n", "    norm_s_2 = tf.pow(norm_s, 2)\n", "    v = (tf.multiply(norm_s_2,s))/(tf.multiply(1+norm_s_2, norm_s))\n", "    return(v)\n", "\n", "def route(n_iter, batch_item_cnt, v, b_prior, u_hat_stopped, aggr, enable_b_prior_update):\n", "    for i in range(n_iter):\n", "        with tf.name_scope(\"Iteration_{}\".format(i)):\n", "            s, c = get_s(b_prior, u_hat_stopped)\n", "            v = get_v(s)\n", "            aggr = tf.reduce_sum(tf.multiply(v[batch_item_cnt, :, :], u_hat_stopped[batch_item_cnt, :, :]),\n", "                                 axis=-1, keepdims=True)\n", "            aggr = tf.expand_dims(aggr, axis=0)\n", "            aggr = tf.multiply(aggr, enable_b_prior_update)\n", "            b_prior = tf.add(b_prior, aggr)\n", "            \n", "    return (v, b_prior, aggr)\n", "\n", "def update_prior(batch_item_cnt, v, b_prior, u_hat_stopped, aggr, enable_b_prior_update):\n", "    v, b_prior, aggr = route(2, batch_item_cnt, v, b_prior, u_hat_stopped, aggr, enable_b_prior_update)\n", "    return([tf.add(batch_item_cnt, 1), v, b_prior, u_hat_stopped, aggr, enable_b_prior_update])"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "a4dcde0e-e03f-436f-8c0c-5e3a7d908e9a", "_uuid": "946702223fb26fe0a4493cd83fd268324c2b0fb0"}, "source": ["# Build CapsNet architecture\n", "32 layer Capsules each with (6, 6) 8D vectors"]}, {"cell_type": "code", "metadata": {"scrolled": true, "_cell_guid": "e25a243a-4e1f-40ea-b588-3d3946bfb6d2", "collapsed": true, "_uuid": "d2dcfc87f3d01b15bfb4d0e9397b28cfab0c970c"}, "outputs": [], "execution_count": null, "source": ["def build_graph(batch_size, is_train=True, add_summaries=False, print_shape=False):\n", "\n", "    graph = tf.Graph()\n", "    end_points = dict()\n", "\n", "    with graph.as_default():\n", "        X = tf.placeholder(shape=[batch_size, 28, 28, 1], name=\"X\", dtype=tf.float32)\n", "        end_points[\"X\"] = X\n", "        if is_train:\n", "            Y = tf.placeholder(shape=[batch_size], name=\"Y\", dtype=tf.int32)\n", "            end_points[\"Y\"] = Y\n", "            Y_one_hot = tf.one_hot(indices=Y, depth=10)\n", "            end_points[\"Y_one_hot\"] = Y_one_hot\n", "        \n", "            enable_b_prior_update = tf.placeholder(shape = [], dtype=tf.float32)\n", "            end_points[\"enable_b_prior_update\"] = enable_b_prior_update\n", "        #First Layer\n", "        with tf.name_scope(\"Conv_Layer_1\"):\n", "\n", "            conv_1 = tf.contrib.layers.conv2d(inputs=X, \n", "                                              num_outputs=256,  \n", "                                              kernel_size=9, \n", "                                              stride=1, padding=\"VALID\", \n", "                                              activation_fn=tf.nn.relu, \n", "                                              weights_initializer=tf.contrib.layers.xavier_initializer_conv2d(), \n", "                                              biases_initializer=tf.zeros_initializer())\n", "\n", "\n", "            end_points[\"conv_layer_1_act\"] = conv_1\n", "\n", "        capsules = list()\n", "        with tf.variable_scope(\"Capsule_Layer\"):\n", "            for n in range(32):\n", "                cap_1 = tf.contrib.layers.conv2d(inputs=conv_1, \n", "                                                 num_outputs=8, kernel_size=9, \n", "                                                 activation_fn=None,\n", "                                                 stride=2, padding=\"VALID\" ,\n", "                                                 weights_initializer=tf.random_uniform_initializer(minval=-0.1, \n", "                                                                                                   maxval=0.1),\n", "                                                 biases_initializer=tf.zeros_initializer())\n", "\n", "                end_points[\"capsule_{}_act\".format(n)] = cap_1\n", "                capsules.append(tf.expand_dims(cap_1, axis=3))\n", "\n", "        with tf.name_scope(\"Transformation\"):\n", "            u = tf.concat(capsules, axis=3)\n", "            end_points[\"u_b_r\"] = u\n", "\n", "            u = tf.reshape(u, shape=[-1, 1152, 8])\n", "            u = tf.expand_dims(tf.expand_dims(u, axis=2), axis=2)\n", "            end_points[\"u\"] = u\n", "\n", "            W = tf.get_variable(shape=[1, 1152, 10, 16, 8], name=\"W_t\", \n", "                                initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1))\n", "            end_points[\"W_t\"] = W\n", "           \n", "            u_hat =tf.reduce_sum(tf.multiply(u, W), axis=4)\n", "            end_points[\"u_hat\"] = u_hat\n", "\n", "            b_prior= tf.get_variable(shape=[1, 1152, 10, 1], name=\"b_prior\", initializer=tf.zeros_initializer(), \n", "                                      trainable=False)\n", "            end_points[\"b_prior\"] = b_prior\n", "           \n", "            aggr = tf.constant(value=0, shape=[1, 1152, 10, 1], dtype=tf.float32, name=\"aggr\")\n", "            \n", "            s, c = get_s(b_prior, u_hat)\n", "            end_points[\"s\"],  end_points[\"c\"] = s, c\n", "            \n", "            v = get_v(s)\n", "            end_points[\"v\"] = v\n", "\n", "        if is_train:\n", "            with tf.name_scope(\"Routing\"):\n", "\n", "                #Routing\n", "                batch_item_cnt = tf.constant(0)\n", "                end_points[\"batch_item_cnt_before\"] = batch_item_cnt  \n", "                               \n", "                u_hat_stopped = tf.stop_gradient(u_hat) \n", "                \n", "                end_of_batch = lambda batch_item_cnt, v, b_prior, u_hat_stopped, aggr, enable_b_prior_update: \\\n", "                                        tf.less(batch_item_cnt, batch_size)\n", "                route_op = tf.while_loop(body=update_prior, cond=end_of_batch, \n", "                                         loop_vars=[batch_item_cnt, \n", "                                                    v, b_prior, u_hat_stopped, aggr, enable_b_prior_update])\n", "                end_points[\"route_op\"] = route_op\n", "\n", "                [batch_item_cnt, _, b_prior_updated, _, aggr, enable_b_prior_update] = route_op\n", "                end_points[\"aggr\"] = aggr\n", "                end_points[\"batch_item_cnt_after\"] = batch_item_cnt\n", "             \n", "                b_prior_op = tf.assign(b_prior, b_prior_updated)\n", "                \n", "                s, c = get_s(b_prior_op, u_hat)\n", "                v = get_v(s)\n", "                \n", "                end_points[\"s_routed\"] = s\n", "                end_points[\"c_routed\"] = c\n", "                end_points[\"b_prior_routed\"] = b_prior_op\n", "                end_points[\"u_hat_routed\"] = u_hat\n", "                end_points[\"v_routed\"] = v\n", "             \n", "            with tf.name_scope(\"Prepare_for_FC\"):\n", "                Y_one_hot_ex = tf.expand_dims(Y_one_hot, axis=1)\n", "                end_points[\"Y_one_hot_ex\"] = Y_one_hot_ex\n", "\n", "                v_masked = tf.squeeze(tf.matmul(Y_one_hot_ex, v))\n", "                end_points[\"v_masked\"] = v_masked\n", "\n", "            with tf.name_scope(\"Fully_connected\"):\n", "                fc_1 = tf.contrib.layers.fully_connected(inputs=v_masked, num_outputs=512, \n", "                                                         activation_fn=tf.nn.relu,\n", "                                                         weights_initializer=tf.contrib.layers.xavier_initializer(), \n", "                                                         biases_initializer=tf.zeros_initializer())\n", "\n", "                end_points[\"fc_1\"] = fc_1\n", "\n", "                fc_2 = tf.contrib.layers.fully_connected(inputs=fc_1, num_outputs=1024, \n", "                                                         activation_fn=tf.nn.relu,\n", "                                                         weights_initializer=tf.contrib.layers.xavier_initializer(), \n", "                                                         biases_initializer=tf.zeros_initializer())\n", "                end_points[\"fc_2\"] = fc_2\n", "\n", "                fc_out = tf.contrib.layers.fully_connected(inputs=fc_2, num_outputs=784, \n", "                                                         activation_fn=tf.nn.sigmoid,\n", "                                                         weights_initializer=tf.contrib.layers.xavier_initializer(), \n", "                                                         biases_initializer=tf.zeros_initializer())\n", "                end_points[\"fc_out\"] = fc_out\n", "        \n", "        with tf.name_scope(\"Predictions\"):\n", "            v_norm = tf.norm(v, axis=2)\n", "            end_points[\"v_norm\"] = v_norm\n", "            predicted_labels = tf.argmax(v_norm, axis=1, output_type=tf.int32)\n", "            end_points[\"predicted_labels\"] = predicted_labels\n", "            \n", "        if is_train:\n", "\n", "            with tf.name_scope(\"Calculate_Losses\"):\n", "                lambd_a = 0.5\n", "                scaler = 0.0005\n", "\n", "                margin_loss_present = tf.multiply(tf.pow(tf.maximum(0., 0.9 - v_norm), 2), Y_one_hot)\n", "                end_points[\"margin_loss_present\"] = margin_loss_present\n", "\n", "                margin_loss_not_present = lambd_a * tf.multiply(tf.pow(tf.maximum(0., v_norm - 0.1), 2), (1 - Y_one_hot))\n", "                end_points[\"margin_loss_not_present\"] = margin_loss_not_present\n", "\n", "                total_margin_loss = tf.reduce_sum(tf.add(margin_loss_present, margin_loss_not_present), axis=1)\n", "                end_points[\"total_margin_loss\"] = total_margin_loss\n", "\n", "                reconstruction_loss = tf.multiply(scaler, \n", "                                                  tf.reduce_sum(\n", "                                                      tf.squared_difference(\n", "                                                          tf.reshape(X, shape=[-1, 784]), fc_out), axis=1))\n", "                end_points[\"reconstruction_loss\"] = reconstruction_loss\n", "\n", "                loss = tf.reduce_mean(tf.add(total_margin_loss, reconstruction_loss))\n", "                end_points[\"loss\"] = loss\n", "\n", "            with tf.name_scope(\"Calculate_Accuracy\"):\n", "                no_of_corrects = tf.cast(tf.reduce_sum(tf.cast(tf.equal(predicted_labels, Y), tf.int32)), tf.float32)\n", "                accuracy = no_of_corrects/batch_size\n", "                end_points[\"accuracy\"] = accuracy\n", "\n", "            if add_summaries:\n", "                tf.summary.scalar(\"loss_avg\", loss)\n", "                tf.summary.scalar(\"accuracy\", accuracy)\n", "\n", "            optimizer = tf.train.AdamOptimizer()\n", "            grads = optimizer.compute_gradients(loss)\n", "            end_points[\"grads\"] = grads\n", "            optimize = optimizer.apply_gradients(grads)\n", "            end_points[\"optimize\"] = optimize\n", "\n", "            if add_summaries:\n", "                summaries_merged = tf.summary.merge_all()\n", "                end_points[\"summaries_merged\"] = summaries_merged\n", "\n", "            init = tf.global_variables_initializer()\n", "            end_points[\"init\"] = init\n", "\n", "        if print_shape:\n", "            for k in end_points.keys():\n", "                k_shape=\"NA\"\n", "                try:\n", "                    k_shape = str(end_points[k].shape)\n", "                except AttributeError:\n", "                    k_shape = type(end_points[k])\n", "\n", "                print(\"\\r {} shape={}\".format(k, k_shape))\n", "\n", "        return(graph, end_points)"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "5954f6ed-de9a-462a-a4e0-bec4aa9f9240", "_uuid": "f610130d72321291f32714a651106c8865a8b67e"}, "source": ["## Load Mnist data"]}, {"cell_type": "code", "metadata": {"_cell_guid": "a39dd8bd-0b79-4cf7-81ea-2cc7f2a735b2", "collapsed": true, "_uuid": "91ab1ee6917fb2932c46aab05802f4c3539aabd2"}, "outputs": [], "execution_count": null, "source": ["data = np.load(\"../input/mnist/mnist.npz\")\n", "x_train, y_train, x_test, y_test = data[\"x_train\"], data[\"y_train\"], data[\"x_test\"], data[\"y_test\"]\n", "x_train, x_test = x_train/255., x_test/255."]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "fb5cadf1-2290-435e-96d0-edfa88ceaef2", "_uuid": "a124386a47676d54d87b5920bdfc99e516d87afb"}, "source": ["## Training CapsNet\n", "It take lots of time to train on this kernel.  So i have not included the code for training. I trained it on GPU and included the trained model."]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "d1b0d722-2862-4010-86ba-8abb8e7a007a", "_uuid": "345c15b09c51614245251246cb59a089523c9722"}, "source": ["# Prediction on test set\n", "Size of the test set is reduced to 1000"]}, {"cell_type": "code", "metadata": {"scrolled": true, "_cell_guid": "9c5f0339-da6e-4469-8d94-cd18a1cbe16a", "_uuid": "cf08fee57cae42eef016c559cfc2bb6fe8ad0945"}, "outputs": [], "execution_count": null, "source": ["tf.reset_default_graph()\n", "\n", "batch_size = 500\n", "graph, end_points = build_graph(batch_size, is_train=False, add_summaries=False, print_shape=False)\n", "\n", "X = end_points[\"X\"]\n", "predicted_labels = end_points[\"predicted_labels\"]\n", "\n", "trainable_variables = np.load(file=\"../input/trained-variables/trainable_variables.npy\")\n", "trainable_variables = [v for v in graph.get_collection(\"variables\") if v.name in trainable_variables]\n", "\n", "#Limit the size of x_test\n", "\n", "if x_test.shape[0] == 10000:\n", "    np.random.seed(10)\n", "    s = np.random.randint(0, 10000, 1000)\n", "    x_test, y_test = x_test[s,...], y_test[s]\n", "test_size = x_test.shape[0]\n", "\n", "n_loops = int(test_size/batch_size)\n", "assert(test_size/batch_size == n_loops)\n", "test_batches = np.split(np.arange(test_size), n_loops)\n", "corrects = 0\n", "\n", "\n", "with tf.Session(graph=graph) as sess:\n", "    restorer = tf.train.Saver(trainable_variables)\n", "    restorer.restore(sess, \"../input/trained-model/capsnet\")\n", "    batch_cnt = 1\n", "    for idxs in test_batches:\n", "        data, lbls = x_test[idxs,...], y_test[idxs]\n", "        data = np.expand_dims(data, axis=-1)\n", "        predicted_lbls = sess.run(end_points[\"predicted_labels\"], feed_dict={X:data})\n", "        batch_correct = np.sum(np.equal(lbls, predicted_lbls))\n", "        \n", "        print(\"----- Batch {} accuracy={} -----\".format(batch_cnt, batch_correct/idxs.shape[0]))\n", "        corrects += batch_correct\n", "        batch_cnt += 1\n", "       \n", "print(\"Accuracy on test set: {}\".format(corrects/test_size))"]}, {"cell_type": "code", "metadata": {"collapsed": true}, "outputs": [], "execution_count": null, "source": []}], "metadata": {"language_info": {"codemirror_mode": {"version": 3, "name": "ipython"}, "version": "3.6.4", "mimetype": "text/x-python", "nbconvert_exporter": "python", "file_extension": ".py", "pygments_lexer": "ipython3", "name": "python"}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}}, "nbformat": 4, "nbformat_minor": 1}
